# ROS2ì¸ì‹

> ìƒìœ„: [[ROS2]]

ì»´í“¨í„° ë¹„ì „ê³¼ ì„¼ì„œ ìœµí•©ì„ í†µí•œ ROS2 ê¸°ë°˜ ì¸ì‹ ì‹œìŠ¤í…œ êµ¬ì¶•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

---

## ğŸ‘ï¸ ROS2 ì¸ì‹ ê°œìš”

### í•µì‹¬ ê¸°ëŠ¥
- **Computer Vision**: OpenCV ê¸°ë°˜ ì˜ìƒ ì²˜ë¦¬
- **3D Perception**: Point Cloud ì²˜ë¦¬ ë° ë¶„ì„
- **Object Detection**: ê°ì²´ ì¸ì‹ ë° ë¶„ë¥˜
- **Sensor Fusion**: ë‹¤ì¤‘ ì„¼ì„œ í†µí•©

### ì£¼ìš” íŒ¨í‚¤ì§€
- **vision_opencv**: OpenCV-ROS2 ì¸í„°í˜ì´ìŠ¤
- **cv_bridge**: ì´ë¯¸ì§€ ë©”ì‹œì§€ ë³€í™˜
- **image_pipeline**: ì´ë¯¸ì§€ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
- **perception_pcl**: Point Cloud Library í†µí•©

---

## ğŸŒ‰ cv_bridge ì‚¬ìš©

### ì´ë¯¸ì§€ ë©”ì‹œì§€ ë³€í™˜
```cpp
#include <cv_bridge/cv_bridge.h>
#include <opencv2/opencv.hpp>

// ROS2 ì´ë¯¸ì§€ â†’ OpenCV Mat
void imageCallback(const sensor_msgs::msg::Image::SharedPtr msg)
{
  try {
    cv_bridge::CvImagePtr cv_ptr = cv_bridge::toCvCopy(msg, "bgr8");
    cv::Mat image = cv_ptr->image;
    
    // OpenCV ì²˜ë¦¬
    cv::Mat processed_image;
    cv::GaussianBlur(image, processed_image, cv::Size(15, 15), 0);
    
    // OpenCV Mat â†’ ROS2 ì´ë¯¸ì§€
    sensor_msgs::msg::Image::SharedPtr output_msg = 
      cv_bridge::CvImage(msg->header, "bgr8", processed_image).toImageMsg();
    
    publisher_->publish(*output_msg);
  }
  catch (cv_bridge::Exception& e) {
    RCLCPP_ERROR(this->get_logger(), "cv_bridge exception: %s", e.what());
  }
}
```

### Python cv_bridge
```python
import cv2
from cv_bridge import CvBridge
import numpy as np

class ImageProcessor(Node):
    def __init__(self):
        super().__init__('image_processor')
        self.bridge = CvBridge()
        
        self.subscription = self.create_subscription(
            Image, '/camera/image_raw', self.image_callback, 10)
        self.publisher = self.create_publisher(Image, '/processed_image', 10)
    
    def image_callback(self, msg):
        try:
            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
            
            # OpenCV ì²˜ë¦¬
            processed = cv2.GaussianBlur(cv_image, (15, 15), 0)
            
            # ROS2 ë©”ì‹œì§€ë¡œ ë³€í™˜
            output_msg = self.bridge.cv2_to_imgmsg(processed, "bgr8")
            output_msg.header = msg.header
            
            self.publisher.publish(output_msg)
        except Exception as e:
            self.get_logger().error(f'Error: {e}')
```

---

## ğŸ“· ì¹´ë©”ë¼ ì¸í„°í˜ì´ìŠ¤

### USB ì¹´ë©”ë¼ ì‚¬ìš©
```bash
# USB ì¹´ë©”ë¼ ë…¸ë“œ ì‹¤í–‰
ros2 run usb_cam usb_cam_node_exe

# ì¹´ë©”ë¼ ì„¤ì •
ros2 param set /usb_cam image_width 640
ros2 param set /usb_cam image_height 480
ros2 param set /usb_cam framerate 30
```

### RealSense ì¹´ë©”ë¼
```bash
# RealSense ë…¸ë“œ ì‹¤í–‰
ros2 launch realsense2_camera rs_launch.py

# ê¹Šì´ ì¹´ë©”ë¼ í™œì„±í™”
ros2 launch realsense2_camera rs_launch.py \
  enable_depth:=true \
  enable_color:=true \
  enable_pointcloud:=true
```

### ì¹´ë©”ë¼ ìº˜ë¦¬ë¸Œë ˆì´ì…˜
```bash
# ì¹´ë©”ë¼ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹¤í–‰
ros2 run camera_calibration cameracalibrator \
  --size 8x6 \
  --square 0.108 \
  image:=/camera/image_raw \
  camera:=/camera
```

---

## ğŸ” ê°ì²´ ì¸ì‹

### ì»¬ëŸ¬ ê¸°ë°˜ ê°ì²´ ê²€ì¶œ
```cpp
void detectColoredObjects(const cv::Mat& image, cv::Mat& output)
{
  cv::Mat hsv_image;
  cv::cvtColor(image, hsv_image, cv::COLOR_BGR2HSV);
  
  // ë¹¨ê°„ìƒ‰ ê°ì²´ ê²€ì¶œ (HSV ë²”ìœ„)
  cv::Scalar lower_red1(0, 50, 50);
  cv::Scalar upper_red1(10, 255, 255);
  cv::Scalar lower_red2(170, 50, 50);
  cv::Scalar upper_red2(180, 255, 255);
  
  cv::Mat mask1, mask2, red_mask;
  cv::inRange(hsv_image, lower_red1, upper_red1, mask1);
  cv::inRange(hsv_image, lower_red2, upper_red2, mask2);
  cv::bitwise_or(mask1, mask2, red_mask);
  
  // ì»¨íˆ¬ì–´ ì°¾ê¸°
  std::vector<std::vector<cv::Point>> contours;
  cv::findContours(red_mask, contours, cv::RETR_EXTERNAL, cv::CHAIN_APPROX_SIMPLE);
  
  // ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
  output = image.clone();
  for (const auto& contour : contours) {
    if (cv::contourArea(contour) > 500) {  // ìµœì†Œ í¬ê¸° í•„í„°
      cv::Rect bbox = cv::boundingRect(contour);
      cv::rectangle(output, bbox, cv::Scalar(0, 255, 0), 2);
    }
  }
}
```

### ArUco ë§ˆì»¤ ê²€ì¶œ
```cpp
#include <opencv2/aruco.hpp>

void detectArUcoMarkers(const cv::Mat& image, cv::Mat& output)
{
  std::vector<int> markerIds;
  std::vector<std::vector<cv::Point2f>> markerCorners;
  
  cv::Ptr<cv::aruco::Dictionary> dictionary = 
    cv::aruco::getPredefinedDictionary(cv::aruco::DICT_6X6_250);
  
  cv::aruco::detectMarkers(image, dictionary, markerCorners, markerIds);
  
  output = image.clone();
  if (markerIds.size() > 0) {
    cv::aruco::drawDetectedMarkers(output, markerCorners, markerIds);
  }
}
```

---

## â˜ï¸ Point Cloud ì²˜ë¦¬

### PCL í†µí•©
```cpp
#include <pcl/point_cloud.h>
#include <pcl/point_types.h>
#include <pcl_conversions/pcl_conversions.h>
#include <pcl/filters/voxel_grid.h>

void pointCloudCallback(const sensor_msgs::msg::PointCloud2::SharedPtr msg)
{
  // ROS2 PointCloud2 â†’ PCL PointCloud
  pcl::PointCloud<pcl::PointXYZ>::Ptr cloud(new pcl::PointCloud<pcl::PointXYZ>);
  pcl::fromROSMsg(*msg, *cloud);
  
  // Voxel Grid í•„í„°ë§
  pcl::PointCloud<pcl::PointXYZ>::Ptr filtered_cloud(new pcl::PointCloud<pcl::PointXYZ>);
  pcl::VoxelGrid<pcl::PointXYZ> voxel_filter;
  voxel_filter.setInputCloud(cloud);
  voxel_filter.setLeafSize(0.01f, 0.01f, 0.01f);  // 1cm ë³µì…€
  voxel_filter.filter(*filtered_cloud);
  
  // PCL PointCloud â†’ ROS2 PointCloud2
  sensor_msgs::msg::PointCloud2 output_msg;
  pcl::toROSMsg(*filtered_cloud, output_msg);
  output_msg.header = msg->header;
  
  publisher_->publish(output_msg);
}
```

### í‰ë©´ ê²€ì¶œ (RANSAC)
```cpp
#include <pcl/segmentation/sac_segmentation.h>

void detectPlane(const pcl::PointCloud<pcl::PointXYZ>::Ptr& cloud)
{
  pcl::ModelCoefficients::Ptr coefficients(new pcl::ModelCoefficients);
  pcl::PointIndices::Ptr inliers(new pcl::PointIndices);
  
  pcl::SACSegmentation<pcl::PointXYZ> seg;
  seg.setOptimizeCoefficients(true);
  seg.setModelType(pcl::SACMODEL_PLANE);
  seg.setMethodType(pcl::SAC_RANSAC);
  seg.setDistanceThreshold(0.01);  // 1cm ì„ê³„ê°’
  
  seg.setInputCloud(cloud);
  seg.segment(*inliers, *coefficients);
  
  if (inliers->indices.size() > 0) {
    RCLCPP_INFO(rclcpp::get_logger("perception"), 
                "Plane detected with %zu inliers", inliers->indices.size());
  }
}
```

---

## ğŸ·ï¸ í‘œì¤€ ë©”ì‹œì§€ íƒ€ì…

### vision_msgs ì‚¬ìš©
```cpp
#include <vision_msgs/msg/detection2_d_array.hpp>
#include <vision_msgs/msg/detection2_d.hpp>

void publishDetections(const std::vector<cv::Rect>& bboxes, 
                      const std::vector<std::string>& labels)
{
  vision_msgs::msg::Detection2DArray detection_array;
  detection_array.header.stamp = this->now();
  detection_array.header.frame_id = "camera_frame";
  
  for (size_t i = 0; i < bboxes.size(); ++i) {
    vision_msgs::msg::Detection2D detection;
    
    // ë°”ìš´ë”© ë°•ìŠ¤ ì„¤ì •
    detection.bbox.center.x = bboxes[i].x + bboxes[i].width / 2.0;
    detection.bbox.center.y = bboxes[i].y + bboxes[i].height / 2.0;
    detection.bbox.size_x = bboxes[i].width;
    detection.bbox.size_y = bboxes[i].height;
    
    // ê°ì²´ ê°€ì„¤ ì¶”ê°€
    vision_msgs::msg::ObjectHypothesisWithPose hypothesis;
    hypothesis.hypothesis.class_id = labels[i];
    hypothesis.hypothesis.score = 0.9;  // ì‹ ë¢°ë„
    detection.results.push_back(hypothesis);
    
    detection_array.detections.push_back(detection);
  }
  
  detection_publisher_->publish(detection_array);
}
```

---

## ğŸ¤– AI/ML í†µí•©

### YOLO ê°ì²´ ê²€ì¶œ
```python
import cv2
import numpy as np
from ultralytics import YOLO

class YOLODetector(Node):
    def __init__(self):
        super().__init__('yolo_detector')
        self.model = YOLO('yolov8n.pt')  # YOLOv8 nano ëª¨ë¸
        
        self.subscription = self.create_subscription(
            Image, '/camera/image_raw', self.image_callback, 10)
        self.publisher = self.create_publisher(
            Detection2DArray, '/detections', 10)
    
    def image_callback(self, msg):
        cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
        
        # YOLO ì¶”ë¡ 
        results = self.model(cv_image)
        
        # ê²€ì¶œ ê²°ê³¼ ì²˜ë¦¬
        detection_array = Detection2DArray()
        detection_array.header = msg.header
        
        for result in results:
            boxes = result.boxes
            if boxes is not None:
                for box in boxes:
                    detection = Detection2D()
                    # ë°”ìš´ë”© ë°•ìŠ¤ ë° í´ë˜ìŠ¤ ì •ë³´ ì„¤ì •
                    # ... ì„¸ë¶€ êµ¬í˜„
                    detection_array.detections.append(detection)
        
        self.publisher.publish(detection_array)
```

---

## ğŸ”„ ì„¼ì„œ ìœµí•©

### ì¹´ë©”ë¼-ë¼ì´ë‹¤ ìœµí•©
```cpp
#include <message_filters/subscriber.h>
#include <message_filters/sync_policies/approximate_time.h>

class SensorFusion : public rclcpp::Node
{
  typedef message_filters::sync_policies::ApproximateTime<
    sensor_msgs::msg::Image, 
    sensor_msgs::msg::PointCloud2> SyncPolicy;

public:
  SensorFusion() : Node("sensor_fusion")
  {
    image_sub_.subscribe(this, "/camera/image_raw");
    pointcloud_sub_.subscribe(this, "/lidar/points");
    
    sync_ = std::make_shared<message_filters::Synchronizer<SyncPolicy>>(
      SyncPolicy(10), image_sub_, pointcloud_sub_);
    sync_->registerCallback(
      std::bind(&SensorFusion::syncCallback, this, 
                std::placeholders::_1, std::placeholders::_2));
  }

private:
  void syncCallback(const sensor_msgs::msg::Image::ConstSharedPtr& image_msg,
                   const sensor_msgs::msg::PointCloud2::ConstSharedPtr& cloud_msg)
  {
    // ì´ë¯¸ì§€ì™€ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë™ê¸°í™” ì²˜ë¦¬
    // 3D ê°ì²´ ê²€ì¶œ ë° ìœ„ì¹˜ ì¶”ì •
  }

  message_filters::Subscriber<sensor_msgs::msg::Image> image_sub_;
  message_filters::Subscriber<sensor_msgs::msg::PointCloud2> pointcloud_sub_;
  std::shared_ptr<message_filters::Synchronizer<SyncPolicy>> sync_;
};
```

---

## ğŸ“Š ì‹œê°í™” ë° ë””ë²„ê¹…

### RViz2 í”ŒëŸ¬ê·¸ì¸
- **Image View**: ì´ë¯¸ì§€ í‘œì‹œ
- **PointCloud2**: í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì‹œê°í™”
- **MarkerArray**: ê²€ì¶œ ê²°ê³¼ í‘œì‹œ
- **TF**: ì¢Œí‘œê³„ ë³€í™˜ í‘œì‹œ

### rqt ë„êµ¬
```bash
# ì´ë¯¸ì§€ ë·°ì–´
rqt_image_view

# ê·¸ë˜í”„ í”Œë¡¯í„°
rqt_plot /detection_confidence

# ë™ì  ì¬ì„¤ì •
rqt_reconfigure
```

---

## ğŸ”§ ì„±ëŠ¥ ìµœì í™”

### GPU ê°€ì†
```cpp
// OpenCV GPU ëª¨ë“ˆ ì‚¬ìš©
#include <opencv2/cudaimgproc.hpp>

void processWithGPU(const cv::Mat& input, cv::Mat& output)
{
  cv::cuda::GpuMat gpu_input, gpu_output;
  gpu_input.upload(input);
  
  // GPUì—ì„œ ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì²˜ë¦¬
  cv::cuda::GaussianBlur(gpu_input, gpu_output, cv::Size(15, 15), 0);
  
  gpu_output.download(output);
}
```

### ë©€í‹°ìŠ¤ë ˆë”©
```cpp
// ë¹„ë™ê¸° ì²˜ë¦¬
std::future<cv::Mat> processAsync(const cv::Mat& image) {
  return std::async(std::launch::async, [image]() {
    cv::Mat result;
    // ì‹œê°„ ì†Œëª¨ì ì¸ ì²˜ë¦¬
    return result;
  });
}
```

---

## ğŸ“– ì£¼ìš” ì°¸ê³ ë¬¸í—Œ

1. **vision_opencv**: https://github.com/ros-perception/vision_opencv
2. **vision_msgs**: https://github.com/ros-perception/vision_msgs
3. **OpenCV Documentation**: https://docs.opencv.org/
4. **PCL Tutorials**: https://pcl.readthedocs.io/
5. **ROS2 Perception Tutorial**: https://ibrahimmansur4.medium.com/integrating-opencv-with-ros2-a-comprehensive-guide-to-computer-vision-in-robotics-66b97fa2de92